{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d91de22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\abdus samad\\\\Desktop\\\\Churn_ML\\\\Churn_ML_Project'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "090437a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\abdus samad\\\\Desktop\\\\Churn_ML\\\\Churn_ML_Project'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/abdus samad/Desktop/Churn_ML/Churn_ML_Project\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41faa86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e21fe4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    all_params: dict\n",
    "    metric_file_name: Path\n",
    "    target_column: str\n",
    "    mlflow_uri: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "234cfa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Churn_Predictor.constants import *\n",
    "from src.Churn_Predictor.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0fa56894",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, \n",
    "                 config_file_path = CONFIG_FILE_PATH, \n",
    "                 params_file_path = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        self.schema = read_yaml(SCHEMA_FILE_PATH)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.XGBBoost\n",
    "        target_column = list(self.schema.TARGET_COLUMN.keys())[0]\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_data_path=config.test_data_path,\n",
    "            model_path=config.model_path,\n",
    "            all_params=params,\n",
    "            metric_file_name=Path(config.metric_file_name),\n",
    "            target_column=target_column,\n",
    "            mlflow_uri=os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "        )\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b22ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from src.Churn_Predictor import logger\n",
    "from src.Churn_Predictor.utils.common import save_json\n",
    "\n",
    "# Load environment variables at module level\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def evaluate_model(self, actual, pred):\n",
    "        \"\"\"Calculate evaluation metrics\"\"\"\n",
    "        accuracy = accuracy_score(actual, pred)\n",
    "        precision = precision_score(actual, pred)\n",
    "        recall = recall_score(actual, pred)\n",
    "        f1 = f1_score(actual, pred)\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1\n",
    "        }\n",
    "    \n",
    "    def setup_mlflow_auth(self):\n",
    "        \"\"\"Setup MLflow authentication from environment variables\"\"\"\n",
    "        username = os.getenv('MLFLOW_TRACKING_USERNAME')\n",
    "        password = os.getenv('MLFLOW_TRACKING_PASSWORD')\n",
    "        \n",
    "        if not username or not password:\n",
    "            logger.warning(\"MLflow credentials not found in environment. Using local tracking only.\")\n",
    "            return False\n",
    "        \n",
    "        # Set credentials as environment variables for MLflow\n",
    "        os.environ['MLFLOW_TRACKING_USERNAME'] = username\n",
    "        os.environ['MLFLOW_TRACKING_PASSWORD'] = password\n",
    "        \n",
    "        logger.info(f\"MLflow authentication configured for user: {username}\")\n",
    "        return True\n",
    "    \n",
    "    def initiate_model_evaluation(self):\n",
    "        \"\"\"Evaluate model and log to MLflow\"\"\"\n",
    "        logger.info(\"Starting model evaluation\")\n",
    "        \n",
    "        # Load test data and model\n",
    "        logger.info(f\"Loading test data from: {self.config.test_data_path}\")\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "        \n",
    "        logger.info(f\"Loading model from: {self.config.model_path}\")\n",
    "        model = joblib.load(self.config.model_path)\n",
    "\n",
    "        # Prepare test data\n",
    "        test_x = test_data.drop(self.config.target_column, axis=1)\n",
    "        test_y = test_data[self.config.target_column]\n",
    "        \n",
    "        logger.info(f\"Test data shape: {test_x.shape}\")\n",
    "        logger.info(f\"Target column: {self.config.target_column}\")\n",
    "\n",
    "        # Setup MLflow authentication\n",
    "        auth_success = self.setup_mlflow_auth()\n",
    "        \n",
    "        # Set tracking and registry URI\n",
    "        logger.info(f\"Setting MLflow tracking URI: {self.config.mlflow_uri}\")\n",
    "        mlflow.set_tracking_uri(self.config.mlflow_uri)\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        \n",
    "        # Set experiment\n",
    "        mlflow.set_experiment(\"Churn_Prediction_Model_Evaluation\")\n",
    "        \n",
    "        # Get tracking URL type\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        try:\n",
    "            with mlflow.start_run():\n",
    "                logger.info(\"MLflow run started\")\n",
    "                \n",
    "                # Make predictions\n",
    "                pred = model.predict(test_x)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = self.evaluate_model(test_y, pred)\n",
    "                \n",
    "                logger.info(\"=\" * 50)\n",
    "                logger.info(\"MODEL EVALUATION METRICS\")\n",
    "                logger.info(\"=\" * 50)\n",
    "                logger.info(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "                logger.info(f\"Precision: {metrics['precision']:.4f}\")\n",
    "                logger.info(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "                logger.info(f\"F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "                logger.info(\"=\" * 50)\n",
    "\n",
    "                # Log parameters and metrics to MLflow\n",
    "                mlflow.log_params(self.config.all_params)\n",
    "                mlflow.log_metrics(metrics)\n",
    "                \n",
    "                # Save metrics locally\n",
    "                save_json(path=Path(self.config.metric_file_name), data=metrics)\n",
    "                logger.info(f\"Metrics saved to: {self.config.metric_file_name}\")\n",
    "\n",
    "                # Log model to MLflow\n",
    "                if tracking_url_type_store != \"file\":\n",
    "                    # Remote tracking (DagsHub)\n",
    "                    logger.info(\"Logging model to remote MLflow server\")\n",
    "                    mlflow.sklearn.log_model(\n",
    "                        model, \n",
    "                        \"model\",\n",
    "                        registered_model_name=\"Churn_Prediction_Model\"\n",
    "                    )\n",
    "                else:\n",
    "                    # Local tracking\n",
    "                    logger.info(\"Logging model to local MLflow\")\n",
    "                    mlflow.sklearn.log_model(model, \"model\")\n",
    "                \n",
    "                logger.info(\"Model evaluation completed successfully\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during MLflow tracking: {e}\")\n",
    "            logger.warning(\"Continuing without MLflow tracking\")\n",
    "            \n",
    "            # Save metrics locally even if MLflow fails\n",
    "            metrics = self.evaluate_model(test_y, model.predict(test_x))\n",
    "            save_json(path=self.config.metric_file_name, data=metrics)\n",
    "            logger.info(f\"Metrics saved locally to: {self.config.metric_file_name}\")\n",
    "            \n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b112511c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-14 16:38:29,261: INFO: 1851394116: Starting Model Evaluation Pipeline]\n",
      "[2026-02-14 16:38:29,265: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2026-02-14 16:38:29,267: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2026-02-14 16:38:29,272: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2026-02-14 16:38:29,273: INFO: common: created directory at: artifacts]\n",
      "[2026-02-14 16:38:29,274: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[2026-02-14 16:38:29,275: INFO: 2021659470: Starting model evaluation]\n",
      "[2026-02-14 16:38:29,276: INFO: 2021659470: Loading test data from: artifacts/data_transformation/test.csv]\n",
      "[2026-02-14 16:38:29,286: INFO: 2021659470: Loading model from: artifacts/model_trainer/model.joblib]\n",
      "[2026-02-14 16:38:29,291: INFO: 2021659470: Test data shape: (1405, 23)]\n",
      "[2026-02-14 16:38:29,292: INFO: 2021659470: Target column: Churn]\n",
      "[2026-02-14 16:38:29,294: INFO: 2021659470: MLflow authentication configured for user: Maazthepal]\n",
      "[2026-02-14 16:38:29,295: INFO: 2021659470: Setting MLflow tracking URI: https://dagshub.com/Maazthepal/Churn_ML_Project.mlflow]\n",
      "[2026-02-14 16:38:34,496: INFO: 2021659470: MLflow run started]\n",
      "[2026-02-14 16:38:34,529: INFO: 2021659470: ==================================================]\n",
      "[2026-02-14 16:38:34,530: INFO: 2021659470: MODEL EVALUATION METRICS]\n",
      "[2026-02-14 16:38:34,531: INFO: 2021659470: ==================================================]\n",
      "[2026-02-14 16:38:34,533: INFO: 2021659470: Accuracy:  0.7737]\n",
      "[2026-02-14 16:38:34,536: INFO: 2021659470: Precision: 0.5572]\n",
      "[2026-02-14 16:38:34,537: INFO: 2021659470: Recall:    0.7070]\n",
      "[2026-02-14 16:38:34,538: INFO: 2021659470: F1-Score:  0.6232]\n",
      "[2026-02-14 16:38:34,540: INFO: 2021659470: ==================================================]\n",
      "[2026-02-14 16:38:35,639: INFO: common: json file saved at: artifacts\\model_evaluation\\metrics.json]\n",
      "[2026-02-14 16:38:35,641: INFO: 2021659470: Metrics saved to: artifacts\\model_evaluation\\metrics.json]\n",
      "[2026-02-14 16:38:35,642: INFO: 2021659470: Logging model to remote MLflow server]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/14 16:38:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "c:\\Users\\abdus samad\\Desktop\\Churn_ML\\Churn_ML_Project\\venv\\Lib\\site-packages\\mlflow\\models\\model.py:1209: FutureWarning: Saving scikit-learn models in the pickle or cloudpickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization.The recommended safe alternative is the 'skops' format.\n",
      "  flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n",
      "Successfully registered model 'Churn_Prediction_Model'.\n",
      "2026/02/14 16:39:02 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Churn_Prediction_Model, version 1\n",
      "Created version '1' of model 'Churn_Prediction_Model'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-14 16:39:03,229: INFO: 2021659470: Model evaluation completed successfully]\n",
      "üèÉ View run treasured-chimp-34 at: https://dagshub.com/Maazthepal/Churn_ML_Project.mlflow/#/experiments/0/runs/34dc554dd2ae4c4792c755bbd4e323a7\n",
      "üß™ View experiment at: https://dagshub.com/Maazthepal/Churn_ML_Project.mlflow/#/experiments/0\n",
      "[2026-02-14 16:39:04,025: INFO: 1851394116: Model evaluation completed successfully]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    logger.info(\"Starting Model Evaluation Pipeline\")\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation.initiate_model_evaluation()\n",
    "    logger.info(\"Model evaluation completed successfully\")\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Error in model evaluation: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2d068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
