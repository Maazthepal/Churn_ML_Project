{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe603d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebce7093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\abdus samad\\\\Desktop\\\\Churn_ML\\\\Churn_ML_Project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e558c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\abdus samad\\\\Desktop\\\\Churn_ML\\\\Churn_ML_Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('..')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457535c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d0f230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Churn_Predictor.constants import *\n",
    "from src.Churn_Predictor.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bf92ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH,\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path\n",
    "        )\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98a8ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pandas as pd\n",
    "from src.Churn_Predictor import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc725b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "    \n",
    "\n",
    "    def initiate_data_transformation(self):\n",
    "        logger.info(\"Reading data from csv file\")\n",
    "        df = pd.read_csv(self.config.data_path)\n",
    "\n",
    "\n",
    "        logger.info(\"Dropping the 'customerID' column from the dataset\")\n",
    "        df.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "        logger.info(\"Finding Null values in the dataset\")\n",
    "        null_counts = df.isnull().sum()\n",
    "        logger.info(f\"Null value counts:\\n{null_counts}\")\n",
    "\n",
    "        logger.info(\"Converting 'TotalCharges' to numeric, coercing errors to NaN\")\n",
    "        df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "        logger.info(\"Finding Null values in 'TotalCharges' after conversion\")\n",
    "        total_charges_null_count = df['TotalCharges'].isnull().sum()\n",
    "        logger.info(f\"Null value count in 'TotalCharges' after conversion: {total_charges_null_count}\")\n",
    "\n",
    "        logger.info(\"Filling mean_values in 'TotalCharges' Null Rows\")\n",
    "        mean_total_charges = df['TotalCharges'].mean()\n",
    "        df['TotalCharges'].fillna(mean_total_charges, inplace=True)\n",
    "        logger.info(\"Mean value filled in 'TotalCharges' Null Rows\")\n",
    "\n",
    "        logger.info(\"Finding the Duplicated values in the dataset\")\n",
    "        duplicated_count = df.duplicated().sum()\n",
    "        logger.info(f\"Duplicated value count: {duplicated_count}\")\n",
    "\n",
    "        logger.info(\"Dropping the Duplicated values in the dataset\")\n",
    "        df.drop_duplicates(keep='first', inplace=True)\n",
    "        logger.info(\"Duplicated values dropped\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def initiate_data_preprocessing(self, df):\n",
    "        logger.info(\"Preparing data for preproceesing\")\n",
    "        df = df.replace({\n",
    "            'No internet service': 'No',\n",
    "            'No phone service': 'No'\n",
    "            })\n",
    "        \n",
    "        binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', \n",
    "               'PaperlessBilling', 'MultipleLines', 'OnlineSecurity', \n",
    "               'OnlineBackup', 'DeviceProtection', 'TechSupport', \n",
    "               'StreamingTV', 'StreamingMovies']\n",
    "        \n",
    "        multi_cols = ['InternetService', 'Contract', 'PaymentMethod']\n",
    "\n",
    "        le_target = LabelEncoder()\n",
    "        df['Churn'] = le_target.fit_transform(df['Churn'])\n",
    "\n",
    "        for cols in binary_cols:\n",
    "            le = LabelEncoder()\n",
    "            df[cols] = le.fit_transform(df[cols])\n",
    "        \n",
    "        df = pd.get_dummies(data=df, columns=multi_cols, drop_first=True)\n",
    "        logger.info(\"Data preprocessing completed\")\n",
    "\n",
    "        logger.info(f\"Final info of the dataframe after preprocessing: {df.shape}\")\n",
    "        logger.info(f\"Final columns of the dataframe after preprocessing: {df.columns.to_list()}\")\n",
    "        return df\n",
    "\n",
    "    def initiate_train_test_split(self, df):\n",
    "        logger.info(\"Initiating train test split\")\n",
    "        train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Churn'])\n",
    "        logger.info(\"Train test split completed\")\n",
    "\n",
    "        train.to_csv(os.path.join(self.config.root_dir, 'train.csv'), index=False)\n",
    "        test.to_csv(os.path.join(self.config.root_dir, 'test.csv'), index=False)\n",
    "\n",
    "        logger.info(f\"Train and test data saved in {self.config.root_dir}\")\n",
    "        logger.info(f\"Train data shape: {train.shape}\")\n",
    "        logger.info(f\"Test data shape: {test.shape}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5841a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-13 16:26:47,732: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2026-02-13 16:26:47,734: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2026-02-13 16:26:47,738: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2026-02-13 16:26:47,739: INFO: common: created directory at: artifacts]\n",
      "[2026-02-13 16:26:47,740: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2026-02-13 16:26:47,741: INFO: 1367153455: Reading data from csv file]\n",
      "[2026-02-13 16:26:47,781: INFO: 1367153455: Dropping the 'customerID' column from the dataset]\n",
      "[2026-02-13 16:26:47,784: INFO: 1367153455: Finding Null values in the dataset]\n",
      "[2026-02-13 16:26:47,791: INFO: 1367153455: Null value counts:\n",
      "gender              0\n",
      "SeniorCitizen       0\n",
      "Partner             0\n",
      "Dependents          0\n",
      "tenure              0\n",
      "PhoneService        0\n",
      "MultipleLines       0\n",
      "InternetService     0\n",
      "OnlineSecurity      0\n",
      "OnlineBackup        0\n",
      "DeviceProtection    0\n",
      "TechSupport         0\n",
      "StreamingTV         0\n",
      "StreamingMovies     0\n",
      "Contract            0\n",
      "PaperlessBilling    0\n",
      "PaymentMethod       0\n",
      "MonthlyCharges      0\n",
      "TotalCharges        0\n",
      "Churn               0\n",
      "dtype: int64]\n",
      "[2026-02-13 16:26:47,793: INFO: 1367153455: Converting 'TotalCharges' to numeric, coercing errors to NaN]\n",
      "[2026-02-13 16:26:47,800: INFO: 1367153455: Finding Null values in 'TotalCharges' after conversion]\n",
      "[2026-02-13 16:26:47,801: INFO: 1367153455: Null value count in 'TotalCharges' after conversion: 11]\n",
      "[2026-02-13 16:26:47,801: INFO: 1367153455: Filling mean_values in 'TotalCharges' Null Rows]\n",
      "[2026-02-13 16:26:47,803: INFO: 1367153455: Mean value filled in 'TotalCharges' Null Rows]\n",
      "[2026-02-13 16:26:47,804: INFO: 1367153455: Finding the Duplicated values in the dataset]\n",
      "[2026-02-13 16:26:47,813: INFO: 1367153455: Duplicated value count: 22]\n",
      "[2026-02-13 16:26:47,814: INFO: 1367153455: Dropping the Duplicated values in the dataset]\n",
      "[2026-02-13 16:26:47,825: INFO: 1367153455: Duplicated values dropped]\n",
      "[2026-02-13 16:26:47,826: INFO: 1367153455: Preparing data for preproceesing]\n",
      "[2026-02-13 16:26:47,874: INFO: 1367153455: Data preprocessing completed]\n",
      "[2026-02-13 16:26:47,875: INFO: 1367153455: Final info of the dataframe after preprocessing: (7021, 24)]\n",
      "[2026-02-13 16:26:47,876: INFO: 1367153455: Final columns of the dataframe after preprocessing: ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'MonthlyCharges', 'TotalCharges', 'Churn', 'InternetService_Fiber optic', 'InternetService_No', 'Contract_One year', 'Contract_Two year', 'PaymentMethod_Credit card (automatic)', 'PaymentMethod_Electronic check', 'PaymentMethod_Mailed check']]\n",
      "[2026-02-13 16:26:47,876: INFO: 1367153455: Initiating train test split]\n",
      "[2026-02-13 16:26:47,893: INFO: 1367153455: Train test split completed]\n",
      "[2026-02-13 16:26:47,949: INFO: 1367153455: Train and test data saved in artifacts/data_transformation]\n",
      "[2026-02-13 16:26:47,951: INFO: 1367153455: Train data shape: (5616, 24)]\n",
      "[2026-02-13 16:26:47,952: INFO: 1367153455: Test data shape: (1405, 24)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdus samad\\AppData\\Local\\Temp\\ipykernel_11940\\1367153455.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['TotalCharges'].fillna(mean_total_charges, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    df = data_transformation.initiate_data_transformation()\n",
    "    df = data_transformation.initiate_data_preprocessing(df)\n",
    "    data_transformation.initiate_train_test_split(df)\n",
    "except Exception as e:\n",
    "    logger.exception(f\"An error occurred during data transformation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a105887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
